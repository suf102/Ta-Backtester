{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a sentiment analysis model.\n",
    "I am using a dataset of 1.6 million tweets found on Kaggle for the training dataset, its not strictly a data set that is made for stock sentiment analysis. There are some dataset that are restricted to stocks, I hypothesise that the same analysis will be applicable. I will test this by using these restricted datasets for validation. The dataset will not be in the repository as it is too large but I have included a link to it below. I have also drawn on on a medium tutorial on sentiment analysis in pytorch.\n",
    "\n",
    "The dataset: https://www.kaggle.com/datasets/kazanova/sentiment140?resource=download\n",
    "\n",
    "Medium Tutorial: https://bhadreshpsavani.medium.com/tutorial-on-sentimental-analysis-using-pytorch-b1431306a2d7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import json\n",
    "import csv\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    " \n",
    "# for printing out status reports\n",
    "import sys\n",
    "\n",
    "# for data visualization\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPU prioitising apple silicon then nvidia cuda and lastly cpu. \n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = getattr(torch,'has_mps',False)\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull the data from the csv file, \n",
    "\n",
    "df = pd.read_csv('../Sentiment_Analysis_Strategy/training.1600000.processed.noemoticon.csv', header= None,encoding='latin-1' )\n",
    "\n",
    "# We dont need most of hte columns such as tweet author or the date and time. \n",
    "\n",
    "df.drop([1,2,3,4], inplace=True, axis=1)\n",
    "\n",
    "df.columns =['Sentiment','Tweet_text']\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used in the next cell to remove all of the special characters in th tweets\n",
    "\n",
    "def removespecial(tweet):\n",
    "    \n",
    "    #calling alpha because it is returning just the alphabet. \n",
    "    \n",
    "    alpha = \"\"\n",
    "    \n",
    "    for ch in tweet:\n",
    "        if ch not in punctuation:\n",
    "            alpha = alpha + ch\n",
    "            \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we need to remove the special characters from the tweets and we need to fix the words to remove caps etc. we could do this by looping through but it will just take far too long\n",
    "\n",
    "df['Tweet_text'] = df['Tweet_text'].apply(lambda x: x.lower())\n",
    "df['Tweet_text'] = df['Tweet_text'].apply(lambda x: removespecial(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have the tweets in a format we are looking for. \n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we need to take all of the words in the tweets and count the occurrences of each word. this is done so we can enumerate the words.\n",
    "# We need to enumerate the words because our model later will need to take integers as its inputs. First we split the tweets into lists with all of the words\n",
    "df['Tweet_words'] = df['Tweet_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Next we need one super string that has all of the tweets as one string. This step can take a very long time. So we will do it by making the counter work over every entry then adding those instead.\n",
    "\n",
    "df['Tweets_counted'] = df['Tweet_words'].apply(lambda x: Counter(x))\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might seem like a slightly odd way to collect the text of all of the words, but this is a faster method compared to using sum over the text in all tweets\n",
    "# It will split the tweets into groups of 1000 and run Counter over them. Aprox 10 minuites\n",
    "collections = {}\n",
    "divisions = 1600\n",
    "for i in range(divisions):\n",
    "    collections[f\"{i}\"] = df['Tweets_counted'].iloc[int(len(df.index)/divisions * i) : int(len(df.index)/divisions * (i+1))].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then combining those 1600 collections into one large mega collection of all words and how often they appear. \n",
    "\n",
    "word_count = sum(collections.values(),Counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting so the most common words are first\n",
    "\n",
    "sorted_words = word_count.most_common(len(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerating the words in the list by how common they are.\n",
    "\n",
    "Enumerated_words ={w:i+1 for i,(w,c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Enumerated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to encode the reviews themselves, this function will apply 0s if we encounter a word we haven't got, which we shouldn't and the enumeration of the word otherwise.\n",
    "\n",
    "def Encode(tweet):\n",
    "    \n",
    "    encoded_tweet = []\n",
    "    \n",
    "    for word in tweet:\n",
    "        if word not in Enumerated_words:\n",
    "            encoded_tweet.append(0)\n",
    "            \n",
    "        else:\n",
    "            encoded_tweet.append(Enumerated_words[word])\n",
    "    return encoded_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the encoding to the tweets in the dataframe\n",
    "\n",
    "df['Encoded_Tweet'] = df['Tweet_words'].apply(lambda x: Encode(x))\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving because we don't want to loose this progress\n",
    "\n",
    "df.to_csv('../Sentiment_Analysis_Strategy/encoded_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
